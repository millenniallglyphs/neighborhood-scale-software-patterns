<!DOCTYPE html>
<html lang="en"><head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Algorithmic Transparency | Neighborhood Scale Software Patterns</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Algorithmic Transparency" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Context Recommendation algorithms blackbox their internal logic and decision-making systems. We don’t know what kinds of data go in, and have no visibility into why certain decisions come out. We end up fetishising the algorithms and calling them “magic” to dismiss the need to make them legible to users Problem Recommendation algorithms on many popular social media platforms run unchecked – as an audience we have no visibility into what metrics the engineers are optimising for. The engineers themselves don’t fully understand how the algorithms choose what to recommend. The lack of transparency makes it difficult to evaluate why we’re being shown certain content. It hides the fact some algorithms maximise for qualities like emotional outrage, shock value, and political extremism. We lack the agency to evaluate and change the algorithms serving us content. These algorithms also decide what advertisements we see based on the data the platform has access to. As users we have no way of understanding what data is being used, and why certain companies or products are targeting us as potential customers. Solution When a piece of content is recommended by an automated system, it should include an epistemic disclosure message explaining why it was suggested, and what factors went into that decision. Advertisements in particular should bear these disclosures, indicating what data points led to the ad being served (age, gender, race, location, income)" />
<meta property="og:description" content="Context Recommendation algorithms blackbox their internal logic and decision-making systems. We don’t know what kinds of data go in, and have no visibility into why certain decisions come out. We end up fetishising the algorithms and calling them “magic” to dismiss the need to make them legible to users Problem Recommendation algorithms on many popular social media platforms run unchecked – as an audience we have no visibility into what metrics the engineers are optimising for. The engineers themselves don’t fully understand how the algorithms choose what to recommend. The lack of transparency makes it difficult to evaluate why we’re being shown certain content. It hides the fact some algorithms maximise for qualities like emotional outrage, shock value, and political extremism. We lack the agency to evaluate and change the algorithms serving us content. These algorithms also decide what advertisements we see based on the data the platform has access to. As users we have no way of understanding what data is being used, and why certain companies or products are targeting us as potential customers. Solution When a piece of content is recommended by an automated system, it should include an epistemic disclosure message explaining why it was suggested, and what factors went into that decision. Advertisements in particular should bear these disclosures, indicating what data points led to the ad being served (age, gender, race, location, income)" />
<link rel="canonical" href="/neighborhood-scale-software-patterns/pattern/2021/02/12/algorithmic-transparency.html" />
<meta property="og:url" content="/neighborhood-scale-software-patterns/pattern/2021/02/12/algorithmic-transparency.html" />
<meta property="og:site_name" content="Neighborhood Scale Software Patterns" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-02-12T23:38:15-08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Algorithmic Transparency" />
<script type="application/ld+json">
{"url":"/neighborhood-scale-software-patterns/pattern/2021/02/12/algorithmic-transparency.html","headline":"Algorithmic Transparency","dateModified":"2021-02-12T23:38:15-08:00","datePublished":"2021-02-12T23:38:15-08:00","@type":"BlogPosting","description":"Context Recommendation algorithms blackbox their internal logic and decision-making systems. We don’t know what kinds of data go in, and have no visibility into why certain decisions come out. We end up fetishising the algorithms and calling them “magic” to dismiss the need to make them legible to users Problem Recommendation algorithms on many popular social media platforms run unchecked – as an audience we have no visibility into what metrics the engineers are optimising for. The engineers themselves don’t fully understand how the algorithms choose what to recommend. The lack of transparency makes it difficult to evaluate why we’re being shown certain content. It hides the fact some algorithms maximise for qualities like emotional outrage, shock value, and political extremism. We lack the agency to evaluate and change the algorithms serving us content. These algorithms also decide what advertisements we see based on the data the platform has access to. As users we have no way of understanding what data is being used, and why certain companies or products are targeting us as potential customers. Solution When a piece of content is recommended by an automated system, it should include an epistemic disclosure message explaining why it was suggested, and what factors went into that decision. Advertisements in particular should bear these disclosures, indicating what data points led to the ad being served (age, gender, race, location, income)","mainEntityOfPage":{"@type":"WebPage","@id":"/neighborhood-scale-software-patterns/pattern/2021/02/12/algorithmic-transparency.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/neighborhood-scale-software-patterns/assets/main.css"><link type="application/atom+xml" rel="alternate" href="/neighborhood-scale-software-patterns/feed.xml" title="Neighborhood Scale Software Patterns" /></head><body><header class="site-header">

  <div class="wrapper">
    <a class="site-title" rel="author" href="/neighborhood-scale-software-patterns/"><h3>Neighborhood Scale Software Patterns</h3></a>
  </div>
</header>

<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header"><div class="titleFlex">
      <h1>
          Algorithmic Transparency
      </h1>
      <p class="date">
        Feb 12, 2021
      </p>
    </div>
    <ul class="tags"><li ><a href="/neighborhood-scale-software-patterns/tags#algorithms" class="tag">algorithms</a></li><li ><a href="/neighborhood-scale-software-patterns/tags#transparency" class="tag">transparency</a></li></ul>
    <p class="post-meta"></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h2 id="context">Context</h2>
<p>Recommendation algorithms blackbox their internal logic and decision-making systems. We don’t know what kinds of data go in, and have no visibility into why certain decisions come out. We end up fetishising the algorithms and calling them “magic” to dismiss the need to make them legible to users</p>
<h2 id="problem">Problem</h2>
<p>Recommendation algorithms on many popular social media platforms run unchecked – as an audience we have no visibility into what metrics the engineers are optimising for. The engineers themselves don’t fully understand how the algorithms choose what to recommend. The lack of transparency makes it difficult to evaluate why we’re being shown certain content. It hides the fact some algorithms maximise for qualities like emotional outrage, shock value, and political extremism. We lack the agency to evaluate and change the algorithms serving us content.
These algorithms also decide what advertisements we see based on the data the platform has access to. As users we have no way of understanding what data is being used, and why certain companies or products are targeting us as potential customers.</p>
<h2 id="solution">Solution</h2>
<p>When a piece of content is recommended by an automated system, it should include an epistemic disclosure message explaining why it was suggested, and what factors went into that decision.
Advertisements in particular should bear these disclosures, indicating what data points led to the ad being served (age, gender, race, location, income)</p>

  </div><a class="u-url" href="/neighborhood-scale-software-patterns/pattern/2021/02/12/algorithmic-transparency.html" hidden></a>
</article>
      </div>
    </main>

  </body>

</html>